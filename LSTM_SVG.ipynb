{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec39fb3f-0be8-45db-85a3-b1212b10f18d",
   "metadata": {},
   "source": [
    "# RNN for Smoothed Sunspot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e262a69-e869-463a-8a39-d05a937e127c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 15:47:52.355478: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82445405-11db-4268-890e-ade5526d5fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sunspot_Number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1818.001</th>\n",
       "      <td>34.566570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818.004</th>\n",
       "      <td>34.546053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818.007</th>\n",
       "      <td>34.525535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818.010</th>\n",
       "      <td>34.505017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818.012</th>\n",
       "      <td>34.484499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Sunspot_Number\n",
       "Year                    \n",
       "1818.001       34.566570\n",
       "1818.004       34.546053\n",
       "1818.007       34.525535\n",
       "1818.010       34.505017\n",
       "1818.012       34.484499"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/SVG_poly1.csv') # importing data for SVG polyorder = 1\n",
    "X = ((df.Year).values).reshape(-1,1)\n",
    "y = (df.Sunspot_Number).values\n",
    "yerr = np.zeros(len(X))\n",
    "\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True) \n",
    "# removes the unnecessary column\n",
    "df.set_index(\"Year\", inplace=True)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788363be-7f60-4c6c-8099-79a2e9a7ee8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "201ade13-5a48-4c1b-bf5f-7d6b57ae90f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 70000\n",
    "skips = 28 # 1 data point per luna cycle\n",
    "train_data = df.iloc[:limit:skips] \n",
    "validation_data = df.iloc[limit::skips]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c907a1-00a2-46da-a5b3-8ddb4f72204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc70b68-7400-4023-943c-84ebfa249940",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(train_data)\n",
    "scaled_train = scaler.transform(train_data)\n",
    "scaled_test = scaler.transform(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46b360bd-f40b-484c-b486-eb08f1d529c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f8c7d9-c1f9-4767-b179-73c76a68df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define generator\n",
    "n_input = 11 * 12 # 11 years \n",
    "n_features = 1 # we are only using 1 timeseries to make our predictions\n",
    "generator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "304f2642-1efd-4ffe-ac4d-a0a461eb3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = generator[0]\n",
    "#print(f'Given the Array: \\n{X.flatten()}')\n",
    "#print(f'Predict this y: \\n {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fec8f90-8c99-4347-98aa-e9213d8af1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c01a358a-bea8-4a9b-8f29-697433c8072c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 100)               40800     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,901\n",
      "Trainable params: 40,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 15:47:58.932867: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# define model\n",
    "model = Sequential() # adds layers in a sequence\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features))) # maybe try elu?\n",
    "# 100 neurons\n",
    "model.add(Dense(1))\n",
    "# 1 layer\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6394a-eaa0-4232-a9a9-d4e93c3bf307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2368/2368 [==============================] - 142s 59ms/step - loss: 0.0508\n",
      "Epoch 2/100\n",
      "2368/2368 [==============================] - 161s 68ms/step - loss: 0.0064\n",
      "Epoch 3/100\n",
      "1049/2368 [============>.................] - ETA: 1:35 - loss: 0.0035"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "epochs = 100\n",
    "\n",
    "# Define a ModelCheckpoint callback to save the model weights based on validation loss\n",
    "#checkpoint = ModelCheckpoint('best_weights.h5', monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Fit the model to the data, using the ModelCheckpoint callback\n",
    "history = model.fit(generator, epochs=epochs, batch_size=1) #validation_data=(scaled_test,scaled_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de699997-b427-4a5c-8508-4a4262bf2226",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_epoch = model.history.history['loss']\n",
    "#val_loss_per_epoch = model.history.history['val_loss']\n",
    "plt.plot(range(len(loss_per_epoch)),loss_per_epoch);\n",
    "#plt.plot(range(len(val_loss_per_epoch)), val_loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027cad75-5bc7-4aed-9b61-e253e7ac61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_train_batch = scaled_train[-n_input:] \n",
    "# take the last n_input month of values, to make \n",
    "# predictions on the 1st test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c36497-2d27-49ee-99d0-5f17aeb01b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_train_batch = last_train_batch.reshape((1, n_input, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2827ecc5-7f07-4bce-9367-9e7300026262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict(last_train_batch)\n",
    "\n",
    "y_pred = model.predict(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e934267-9323-443b-bf02-7ee1ed36d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred, label='predict')\n",
    "plt.plot(scaled_train, label='train')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a534f6-9e5e-44f1-af70-7b379daee63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_predictions = []\n",
    "\n",
    "first_eval_batch = scaled_train[-n_input:]\n",
    "current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "\n",
    "for i in range(len(validation_data)):\n",
    "    \n",
    "    # get the prediction value for the first batch\n",
    "    current_pred = model.predict(current_batch)[0]\n",
    "    \n",
    "    # append the prediction into the array\n",
    "    validation_predictions.append(current_pred) \n",
    "    \n",
    "    # use the prediction to update the batch and remove the first value\n",
    "    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787881a8-d5ab-42b6-8403-302b605fe40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_predictions\n",
    "# NOTE: these predictions are in the range 0-1\n",
    "# we need to convert it back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1f186-6cc9-4993-83df-16cae238fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_predictions = scaler.inverse_transform(validation_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee137c3-91f5-454e-a4cf-fb9e73275afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data['Predictions'] = true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f1e22-3adb-426a-82e3-f45ce5e004a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.plot(figsize=(14,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01a206-0ccc-419c-bb67-ba52320f2a53",
   "metadata": {},
   "source": [
    "### Comment on plot:\n",
    "Initially started using an RNN with a ReLu activation function, which produced some form of results, but they didn't fit well to the data.\n",
    "\n",
    "I have attempted to use a Sigmoid function instead, which again seems to produce some form of results, but it predicts the sunspot number to be much higher than it was, and doesn't seem to come back down to a minimum of zero. One positive note is th"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
