{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3493ecb-39bb-4b53-a385-5cb5b53a971d",
   "metadata": {},
   "source": [
    "# Notes on Statistics & Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e61ee-41af-4200-b485-8e2a0788fc28",
   "metadata": {},
   "source": [
    "# Statistics Notes\n",
    "##### Sources ~ Numerical Recipes, & Scientific Inference Lecture Notes\n",
    "\n",
    "### Moments of a Distribution\n",
    "* **p-value test**: the p-value tells us how likely this datapoint happened by chance. We set the p-value such that is a data point falls inside the tail region we conclude that the *null hypothesis* is *false*. NB, we cannot prove the null hypothesis, only *disprove* it.\n",
    "\n",
    "* **Model independent data analysis**: descriptive, i.e. *mean*, *variance*, etc.\n",
    "\n",
    "* **Model dependent data analysis**: *parameter estimation*, *least-squares fits* etc.\n",
    "*******************************************************************************************************************************\n",
    "* **Mean**: $  \\langle x \\rangle \\equiv \\overline{x} = \\frac{1}{N} \\sum_{j=0}^{N-1} x_{j} = \\sum_{i} x_i P(x_i) = \\int x P(x) \\,dx $ --> may not be very good for distributions with broad tails.\n",
    "\n",
    "* **Median**: $\\int_{-\\infty}^{x_{med}} p(x)\\,dx = \\frac{1}{2} = \\int_{x_{med}}^{\\infty} p(x)\\,dx$ --> value of PDF in which values smaller or larger than $x_{med}$ are equally probable.\n",
    "\n",
    "$x_{med} = \\begin{cases}\n",
    "    x_{(N+1)/2}, & \\text{N odd}\\\\\n",
    "    \\frac{1}{2}(x_{N/2} + x_{N/2 + 1}), & \\text{N even}\n",
    "  \\end{cases}$\n",
    "\n",
    "* **Mode**: Value of $x$ where $p(x)$ takes on the *greatest* value.\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "* **Variance**: $ Var(x) = \\frac{1}{N-1} \\sum_{j=0}^{N-1} \\left( x_{j} - \\overline{x} \\right)^2 = \\sum_{i} \\left(x_i - \\overline{x} \\right)^2 P(x_i) = \\int \\left(x - \\overline{x} \\right)^2 P(x) \\,dx $ --> width or variability around that value.\n",
    "\n",
    "* **Skewness**: $\\gamma = \\frac{1}{N} \\sum_{j=1}^{N} \\left[\\frac{x_j - \\overline{x}}{\\sigma}\\right]^3$ --> characterises the degree of aysymmetry of a distribution around its mean.\n",
    "\n",
    "* **Kurtosis**: $\\kappa = \\left\\{\\frac{1}{N} \\sum_{j=1}^{N} \\left[\\frac{x_j - \\overline{x}}{\\sigma}\\right]^4\\right\\} - 3$ --> Measures peakedness or flatness of a distribution relative to the *Normal Distribution*.\n",
    "\n",
    "* **Cumulative Distribution Function**: $C(x) = Prob(x' \\le x) = \\sum_{x_i \\le x} P(x_i) = \\int_{x_{min}}^{x} P(x') \\, dx'$\n",
    "\n",
    "Note: for **Discrete Distributions** we have a **PMF** which we *sum* over, and for **Continuous Distributions** we have a **PDF** which we *integrate* over.\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "* **Standard Deviation**: $ \\sigma(x) = \\sqrt{Var(x)}$ --> mean squared deviation of $x$ from its mean value.\n",
    "\n",
    "* **Standard Error of Estimated Mean**: $\\sigma(x) / \\sqrt{N}$ --> accuracy with which the sample mean estimates the population mean.\n",
    "\n",
    "* **Standard Error of Estimated Variance**: $\\sigma(x)^2 \\sqrt{\\frac{2}{N}}$.\n",
    "\n",
    "* **Standard Error of Estimated $\\sigma$**: $\\sigma(x)/\\sqrt{2N}$.\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "* **Moments**: $\\mu_k = \\langle x^k \\rangle = \\int x^k P(x)\\, dx $\n",
    "    * $\\mu_0 = \\int P(x)\\, dx = 1$\n",
    "    * $\\mu_1 = \\langle x \\rangle = \\mu$\n",
    "    \n",
    "* **Central Momements**: $\\nu_k = \\langle (x - \\mu)^k \\rangle = \\int (x - \\mu)^k P(x)\\, dx$\n",
    "    * $Var(x) = \\nu_2$\n",
    "    * **Skewness**: $\\gamma = \\frac{\\nu_3}{Var(x)^{3/2}}$ \n",
    "    * **Kurtosis**: $\\kappa = \\frac{\\nu_4}{Var(x)^2}$\n",
    "\n",
    "* **Moment Generating Function**: $M_x(t) = \\langle \\exp(tx) \\rangle = \\int \\exp(tx) P(x)\\, dx$ --> useful for computing moments.\n",
    "    * $\\mu_k = \\frac{d^k}{dt^k}\\Big|_{t=0} M_x(t)$\n",
    "*******************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952540c7-47e7-4aa3-8cd3-18e66ae31075",
   "metadata": {},
   "source": [
    "### Significantly Different Means/Variances\n",
    "\n",
    "A quantity that measures the significance of a distribution of means is *not* the number of standard deviations they are apart, but the number of ***standard errors*** they are apart (see definition above).\n",
    "\n",
    "* **Student's t-test for Significantly Different Means**: when two distributions are thought to have the **same variance** but **different means**.\n",
    "\n",
    "Standard error of the difference of the means: $ s_D = \\sqrt{\\frac{\\sum_{i \\in A}(x_i - \\overline{x_A})^2 + \\sum_{i \\in B} (x_i - \\overline{x_B})^2}{N_A + N_B - 2} \\left(\\frac{1}{N_A} + \\frac{1}{N_B} \\right)}$, where A & B are two samples.\n",
    "    $t = \\frac{\\overline{x_A} - \\overline{x_B}}{s_D}$, then we evaluate the significance of $t$.\n",
    "\n",
    "* **Covariance**: measures the *joint variability* of two random variables.\n",
    "\n",
    "    $ Cov(x,y) = \\frac{1}{N-1} \\sum_{i=1}^{N}(x_i - \\overline{x})(y_i - \\overline{y})$\n",
    "\n",
    "    $ s_D = \\left[\\frac{Var(x) + Var(y) - 2Cov(x,y)}{N} \\right]^{1/2} $\n",
    "    \n",
    "* **Correlation**: $Corr(x,y) = \\frac{Cov(x,y)}{\\sigma_x \\sigma_y}$\n",
    "\n",
    "* **F-test for Significantly Different Variances**: tests hypothesis that two different variables have different variances by trying to reject the null hypothesis that the variances are the same.\n",
    "\n",
    "*******************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d33251b-b926-43cd-a11c-5cbdd44701ca",
   "metadata": {},
   "source": [
    "### Are Two Distributions Different?\n",
    "Again, we cannot prove that two sets came from the same distribution, but we can disprove that null hypothesis that they are drawn from the same distribution.\n",
    "\n",
    "* Chi-Square Test: $\\chi^2 = \\sum_i \\frac{(O_i - E_i)^2}{E_i}$, where $O_i$ is the observed value, and $E_i$ is the expected value. For $\\chi^2 \\gg 1$, the null hypothesis is *unlikely*.\n",
    "    For two **binned** data: $\\chi^2 = \\sum_i \\frac{(R_i - S_i)^2}{R_i + S_i}$, where $R_i$ is the no. of events in bin i for set 1, and $S_i$ is the number of events in bin i for set 2.\n",
    "    \n",
    "    \n",
    "*******************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2be9e-66f8-411b-946c-ea20a4c09278",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modelling of Data\n",
    "\n",
    "Given a set of observations we want to condense and summarize the data by fitting it to a \"model\" that depends on adjustable parameters.\n",
    "\n",
    "To be useful, a fitting procedure should provide (i) parameters, (ii) error estimates on the parameters, and (iii) a statistical measure of goodness-of-fit.\n",
    "\n",
    "* **Least Squares as a Maximum Likelihood Estimator** \n",
    "\n",
    "    https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\n",
    "    \n",
    "    In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable.\n",
    "\n",
    "    Suppose we are fitting $N$ data points $(x_i,y_i), i=1,...,N$, to a model that has $M$ adjustable parameters $a_j$, $j=1,...,M$. The model predicts a relationship between the measured *independent* and *dependent* variables. $ y(x) = y(x; a_1 ...a_M)$. To get the best fitted values for $a_j$'s (our parameters) we want to ***minimise*** the **least-squares fit**.\n",
    "    \n",
    "    $$ \\sum_i^N \\left[y_i - y(x_i;a_1 ... a_M) \\right]^2 $$\n",
    "    \n",
    "    ***but why?***\n",
    "    What we want to do is maximise the probability that, given a particular set of parameters, this data could have occured. The probability of getting said data is given by the following equation:\n",
    "    \n",
    "    $$ P = \\prod_{i=1}^N \\left \\{\\exp{\\left[- \\frac{1}{2} \\left(\\frac{y_i - y(x_i)}{\\sigma} \\right)^2 \\right]} \\Delta y \\right \\} $$\n",
    "\n",
    "    where $y_i$ is our *observed* value, $y(x_i)$ is our *expected* value, and $\\Delta y$ is our error in $y$. Maximising this probability is the same as maximising its logarithm or *minimizing* the *negative* of its *logarithm*:\n",
    "    $$ \\left[ \\sum_{i=1}^N \\frac{y_i - y(x_i)}{2 \\sigma^2} \\right] - N \\log{\\Delta y} $$\n",
    "    Hence we arrive at the initial point that the *least-squares fitting* is a maximum likelihood estimation.\n",
    "    \n",
    "    What about when we have *outliers*? Because the probability of large outliers occuring is technically very small, when we do see one our **maximum likelihood estimator** tries to fit the data to it, and consequently ruins the model. \n",
    "    \n",
    "* **Chi-Square Fitting**\n",
    "\n",
    "    Here, each data point has its *own* standard deviation, and the chi-square value is given by $$\\chi^2 \\equiv \\sum_i \\left(\\frac{y_i - y(x_i; a_1...a_M)}{\\sigma_i}\\right)^2$$\n",
    "    To normalise it, we divide by the number of degrees of freedom $\\nu = N - M$ (number of data points - number of parameters). A rule of thumb is that a $\\chi^2 \\approx  \\nu$ for a \"moderately\" good fit.\n",
    "\n",
    "* **Fitting Data to a Straight Line**\n",
    "\n",
    "    Fitting $N$ data points $(x_i, y_i)$ to a straight-line model $y(x) = y(x; a,b) = a + bx$, often called ***linear regression***. To measure how well the model agrees with the data, we use the *chi-square* function (where our errors are already known).\n",
    "    $$\\chi^2(a,b) = \\sum_i^N \\left(\\frac{y_i - a - bx_i}{\\sigma_i}\\right)^2$$\n",
    "    \n",
    "    If the errors are *normally distributed* then this will give us the maximum likelihood parameter estimations. If they're not, it will give its best guess.\n",
    "    \n",
    "    The above equation is minimized to determine $a$ and $b$. At its minimum, $\\frac{\\partial \\chi^2}{\\partial a} = \\frac{\\partial \\chi^2}{\\partial b}  = 0$.\n",
    "    \n",
    "    We must also make estimates of the probable uncertainties of $a$ and $b$, since errors in the data must create uncertainty in our values for the parameters.\n",
    "    \n",
    "    The variance in the value of any function will be:\n",
    "    $ \\sigma_f^2 = \\sum_{i=1}^N \\sigma_i^2 \\left(\\frac{\\partial f}{\\partial y_i} \\right)^2 $\n",
    "    \n",
    "* **The Normal Distribution**\n",
    "    \n",
    "     A *continuous* distribution. The normal distribution is given by the equation:\n",
    "    \n",
    "    $$ P(x ; \\mu, \\sigma^2) = \\frac{\\exp{\\left(- \\frac{(x-\\mu)^2}{2 \\sigma^2} \\right)}}{\\sigma \\sqrt{2 \\pi}} $$\n",
    "    \n",
    "    We tend to represent the normal distribution as: $x \\sim \\mathcal{N} (\\mu, \\sigma^2) $, where $x$ is a random variable, $\\mathcal{N}$ is the normal distribution, and $\\mu$ and $\\sigma^2$ are mean and standard deviation respectively. \n",
    "\n",
    "* **The Poisson Distribution**\n",
    "\n",
    "    A *discrete* distribution. Used for modelling the number of independent events that occurs in a fixed interval of time if the events occur at a *constant mean rate*.\n",
    "    \n",
    "    $$ P(n;\\lambda) = \\frac{\\exp{(-\\lambda)}\\lambda^n}{n!} $$\n",
    "    \n",
    "    where the random variable $n$ is the number of events, and $\\lambda > 0$ is the rate \n",
    "\n",
    "* **The Binomial Distribution**\n",
    "    A *discrete* distribution. Describes the number of successes r in a sequence of n independent experiments.\n",
    "    \n",
    "    $$ P(r;n,p) = p^r(1-p)^{n-r} {n \\choose r} $$\n",
    "    \n",
    "    $${n \\choose r} = \\frac{n!}{r!(n-r)!}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5db8c-2197-4e03-9e98-3255b2194469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
